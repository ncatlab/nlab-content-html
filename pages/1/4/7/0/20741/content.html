
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0 plus SVG 1.1//EN" "http://www.w3.org/2002/04/xhtml-math-svg/xhtml-math-svg-flat.dtd" >
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>
      neural network in nLab
  </title>

  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <meta name="robots" content="index,follow" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link href="/stylesheets/instiki.css?1676280126" media="all" rel="stylesheet" type="text/css" />
  <link href="/stylesheets/mathematics.css?1660229990" media="all" rel="stylesheet" type="text/css" />
  <link href="/stylesheets/syntax.css?1660229990" media="all" rel="stylesheet" type="text/css" />
  <link href="/stylesheets/nlab.css?1676280126" media="all" rel="stylesheet" type="text/css" />
  <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/dreampulse/computer-modern-web-font@master/fonts.css"/>

  <style type="text/css">
    h1#pageName, div.info, .newWikiWord a, a.existingWikiWord, .newWikiWord a:hover, [actiontype="toggle"]:hover, #TextileHelp h3 {
      color: #226622;
    }
    a:visited.existingWikiWord {
      color: #164416;
    }
    
    
  </style>
  
  <style type="text/css"><!--/*--><![CDATA[/*><!--*/
    .toc ul {margin: 0; padding: 0;}
.toc ul ul {margin: 0; padding: 0 0 0 10px;}
.toc li > p {margin: 0}
.toc ul li {list-style-type: none; position: relative;}
.toc div {border-top:1px dotted #ccc;}
.rightHandSide h2 {font-size: 1.5em;color:#008B26}
table.plaintable {
    border-collapse:collapse;
    margin-left:30px;
    border:0;
}
.plaintable td {border:1px solid #000; padding: 3px;}
.plaintable th {padding: 3px;}
.plaintable caption {
    font-weight: bold;
    font-size:1.1em;
    text-align:center;
    margin-left:30px;
}
    
   
/* Query boxes for questioning and answering mechanism */
div.query{
background: #f6fff3;
border: solid #ce9;
border-width: 2px 1px;
padding: 0 1em;
margin: 0 1em;
max-height: 20em;
overflow: auto;
}

/* Standout boxes for putting important text */
div.standout{
background: #fff1f1;
border: solid black;
border-width: 2px 1px;
padding: 0 1em;
margin: 0 1em;
overflow: auto;
}

/* Icon for links to n-category arXiv documents 
 (commented out for now i.e. disabled)
a[href*="http://arxiv.org/"] {
  background-image: url(../files/arXiv_icon.gif);
  background-repeat: no-repeat;
  background-position: right bottom;
     padding-right: 22px;
  } */
    
/* Icon for links to n-category cafe posts (disabled) 
a[href*="http://golem.ph.utexas.edu/category"] {
  background-image: url(../files/n-cafe_5.gif);
  background-repeat: no-repeat;
  background-position: right bottom;
     padding-right: 25px;
  } */

/* Icon for links to pdf files (disabled)
a[href$=".pdf"] {
  background-image: url(../files/pdficon_small.gif);
  background-repeat: no-repeat;
  background-position: right bottom;
     padding-right: 25px;
  } */

/* Icon for links to pages, etc. -inside- pdf files (disabled)
a[href*=".pdf#"] {
  background-image: url(../files/pdf_entry.gif);
  background-repeat: no-repeat;
  background-position: right bottom;
     padding-right: 25px;
  } */

a.existingWikiWord {
color: #226622;
}

a.existingWikiWord:visited {
color: #226622;
}


a.existingWikiWord[title] {
border: 0px;
color: #aa0505;
text-decoration: none;
}
    
a.existingWikiWord[title]:visited {
border: 0px;
color: #551111;
text-decoration: none;
}

a[href^="http://"] {
border: 0px;
color: #003399;
}

a[href^="http://"]:visited {
border: 0px;
color: #330066;
}

a[href^="https://"] {
border: 0px;
color: #003399;
}

a[href^="https://"]:visited {
border: 0px;
color: #330066;
}

div.dropDown .hide {
display: none;
}

div.dropDown:hover .hide {
display:block;
}

div.clickDown .hide {
display: none;
}

div.clickDown:focus {
outline:none;
}

div.clickDown:focus .hide, div.clickDown:hover .hide {
display: block;
}

div.clickDown .clickToReveal, div.clickDown:focus .clickToHide {
display:block;
}

div.clickDown:focus .clickToReveal, div.clickDown .clickToHide {
display:none;
}

div.clickDown .clickToReveal:after {
content: "A(Hover to reveal, click to "hold")";
font-size: 60%;
}

div.clickDown .clickToHide:after {
content: "A(Click to hide)";
font-size: 60%;
}
div.clickDown .clickToHide, div.clickDown .clickToReveal {
white-space: pre-wrap;
}

.un_theorem, .num_theorem, .un_lemma, .num_lemma, .un_prop, .num_prop, .un_cor, .num_cor, .un_defn, .num_defn, .un_example, .num_example, .un_note, .num_note, .un_remark, .num_remark {
margin-left: 1em;
}

span.theorem_label {
margin-left: -1em;
}
.proof span.theorem_label {
margin-left: 0em;
}
:target {
    background-color: #BBBBBB;
    border-radius: 5pt;
}

  /*]]>*/--></style>
  <script src="/javascripts/prototype.js?1660229990" type="text/javascript"></script>
<script src="/javascripts/effects.js?1660229990" type="text/javascript"></script>
<script src="/javascripts/dragdrop.js?1660229990" type="text/javascript"></script>
<script src="/javascripts/controls.js?1660229990" type="text/javascript"></script>
<script src="/javascripts/application.js?1660229990" type="text/javascript"></script>
  <script src="/javascripts/page_helper.js?1660229990" type="text/javascript"></script>
  <script src="/javascripts/thm_numbering.js?1660229990" type="text/javascript"></script>
  
  <script type="text/x-mathjax-config">
  <!--//--><![CDATA[//><!--
    MathJax.Ajax.config.path["Contrib"] = "/MathJax";
    MathJax.Hub.Config({
      MathML: { useMathMLspacing: true },
      "HTML-CSS": { scale: 90,
                    extensions: ["handle-floats.js"]
      }
    });
    MathJax.Hub.Queue( function () {
       var fos = document.getElementsByTagName('foreignObject');
       for (var i = 0; i < fos.length; i++) {
         MathJax.Hub.Typeset(fos[i]);
       }
    });
  //--><!]]>
  </script>

  <script type="text/javascript">
    <!--//--><![CDATA[//><!--
    window.addEventListener("DOMContentLoaded", function () {
      var div = document.createElement('div');
      var math = document.createElementNS('http://www.w3.org/1998/Math/MathML', 'math');
      document.body.appendChild(div);
      div.appendChild(math);
    // Test for MathML support comparable to WebKit version https://trac.webkit.org/changeset/203640 or higher.
      div.setAttribute('style', 'font-style: italic');
      var mathml_unsupported = !(window.getComputedStyle(div.firstChild).getPropertyValue('font-style') === 'normal');
      div.parentNode.removeChild(div);
      if (mathml_unsupported) {
        // MathML does not seem to be supported...
        var s = document.createElement('script');
        s.src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=MML_HTMLorMML-full";
        document.querySelector('head').appendChild(s);
      } else {
        document.head.insertAdjacentHTML("beforeend", '<style>svg[viewBox] {max-width: 100%}</style>');
      }
    });
    //--><!]]>
  </script>

  	<link href="https://ncatlab.org/nlab/atom_with_headlines" rel="alternate" title="Atom with headlines" type="application/atom+xml" />
  	<link href="https://ncatlab.org/nlab/atom_with_content" rel="alternate" title="Atom with full content" type="application/atom+xml" />
  <script type="text/javascript">
  document.observe("dom:loaded", function() {
    generateThmNumbers();
  });
  </script>
</head>

<body>

<div id="Container">
<div id="Content">
  <h1 id="pageName">
    <span style="float: left; margin: 0.5em 0.25em -0.25em 0">
      <svg xmlns="http://www.w3.org/2000/svg" width="1.872em" height="1.8em" viewBox="0 0 190 181">
        <path fill="#226622" d="M72.8 145c-1.6 17.3-15.7 10-23.6 20.2-5.6 7.3 4.8 15 11.4 15 11.5-.2 19-13.4 26.4-20.3 3.3-3 8.2-4 11.2-7.2a14 14 0 0 0 2.9-11.1c-1.4-9.6-12.4-18.6-16.9-27.2-5-9.6-10.7-27.4-24.1-27.7-17.4-.3-.4 26 4.7 30.7 2.4 2.3 5.4 4.1 7.3 6.9 1.6 2.3 2.1 5.8-1 7.2-5.9 2.6-12.4-6.3-15.5-10-8.8-10.6-15.5-23-26.2-31.8-5.2-4.3-11.8-8-18-3.7-7.3 4.9-4.2 12.9.2 18.5a81 81 0 0 0 30.7 23c3.3 1.5 12.8 5.6 10 10.7-2.5 5.2-11.7 3-15.6 1.1-8.4-3.8-24.3-21.3-34.4-13.7-3.5 2.6-2.3 7.6-1.2 11.1 2.8 9 12.2 17.2 20.9 20.5 17.3 6.7 34.3-8 50.8-12.1z"/>
        <path fill="#a41e32" d="M145.9 121.3c-.2-7.5 0-19.6-4.5-26-5.4-7.5-12.9-1-14.1 5.8-1.4 7.8 2.7 14.1 4.8 21.3 3.4 12 5.8 29-.8 40.1-3.6-6.7-5.2-13-7-20.4-2.1-8.2-12.8-13.2-15.1-1.9-2 9.7 9 21.2 12 30.1 1.2 4 2 8.8 6.4 10.3 6.9 2.3 13.3-4.7 17.7-8.8 12.2-11.5 36.6-20.7 43.4-36.4 6.7-15.7-13.7-14-21.3-7.2-9.1 8-11.9 20.5-23.6 25.1 7.5-23.7 31.8-37.6 38.4-61.4 2-7.3-.8-29.6-13-19.8-14.5 11.6-6.6 37.6-23.3 49.2z"/>
        <path fill="#193c78" d="M86.3 47.5c0-13-10.2-27.6-5.8-40.4 2.8-8.4 14.1-10.1 17-1 3.8 11.6-.3 26.3-1.8 38 11.7-.7 10.5-16 14.8-24.3 2.1-4.2 5.7-9.1 11-6.7 6 2.7 7.4 9.2 6.6 15.1-2.2 14-12.2 18.8-22.4 27-3.4 2.7-8 6.6-5.9 11.6 2 4.4 7 4.5 10.7 2.8 7.4-3.3 13.4-16.5 21.7-16 14.6.7 12 21.9.9 26.2-5 1.9-10.2 2.3-15.2 3.9-5.8 1.8-9.4 8.7-15.7 8.9-6.1.1-9-6.9-14.3-9-14.4-6-33.3-2-44.7-14.7-3.7-4.2-9.6-12-4.9-17.4 9.3-10.7 28 7.2 35.7 12 2 1.1 11 6.9 11.4 1.1.4-5.2-10-8.2-13.5-10-11.1-5.2-30-15.3-35-27.3-2.5-6 2.8-13.8 9.4-13.6 6.9.2 13.4 7 17.5 12C70.9 34 75 43.8 86.3 47.4z"/>
      </svg>
    </span>
      <span class="webName">nLab</span>
      neural network
  </h1>



<div class="navigation">
    <span class="skipNav"><a href='#navEnd'>Skip the Navigation Links</a> | </span>
    <span style="display:inline-block; width: 0.3em;"></span>
    <a href="/nlab/show/HomePage" accesskey="H" title="Home page">Home Page</a> |
    <a href="/nlab/all_pages" accesskey="A" title="List of all pages">All Pages</a> |
    <a href="/nlab/latest_revisions" accesskey="U" title="Latest edits and page creations">Latest Revisions</a> |
    <a href="https://nforum.ncatlab.org/discussion/10400/#Item_19" title="Discuss this page in its dedicated thread on the nForum" style="color: black">Discuss this page</a> |
<form accept-charset="utf-8" action="/nlab/search" id="navigationSearchForm" method="get">      <fieldset class="search"><input type="text" id="searchField" name="query" value="Search"
             style="display:inline-block; float: left;"
             onfocus="this.value == 'Search' ? this.value = '' : true"
             onblur="this.value == '' ? this.value = 'Search' : true" /></fieldset>
</form>  <span id='navEnd'></span>
</div>






<div id="revision">
    <html xmlns="http://www.w3.org/1999/xhtml" xmlns:svg="http://www.w3.org/2000/svg" xml:lang="en" lang="en">
<head><meta http-equiv="Content-type" content="application/xhtml+xml;charset=utf-8" /><title>Contents</title></head>
<body>
<div class="rightHandSide">
<div class="toc clickDown" tabindex="0">
<h3 id="context">Context</h3>

<h4 id="probability_theory">Probability theory</h4>

<div class="hide"><div>

<p><strong><a class="existingWikiWord" href="/nlab/show/measure+theory">measure theory</a></strong></p>

<p><strong><a class="existingWikiWord" href="/nlab/show/probability+theory">probability theory</a></strong></p>

<p>(<a class="existingWikiWord" href="/nlab/show/quantum+probability">quantum probability</a>)</p>

<h2 id="measure_theory">Measure theory</h2>

<ul>
<li>
<p><a class="existingWikiWord" href="/nlab/show/measurable+space">measurable space</a>, <a class="existingWikiWord" href="/nlab/show/measurable+locale">measurable locale</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/measure">measure</a>, <a class="existingWikiWord" href="/nlab/show/measure+space">measure space</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/von+Neumann+algebra">von Neumann algebra</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/geometric+measure+theory">geometric measure theory</a></p>
</li>
</ul>

<h2 id="probability_theory">Probability theory</h2>

<ul>
<li>
<p><a class="existingWikiWord" href="/nlab/show/probability+space">probability space</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/probability+distribution">probability distribution</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/state">state</a></p>

<ul>
<li>
<p><a class="existingWikiWord" href="/nlab/show/states+in+AQFT+and+operator+algebra">in AQFT and operator algebra</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/GNS+construction">GNS construction</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/Fell%27s+theorem">Fell's theorem</a></p>
</li>
</ul>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/entropy">entropy</a>, <a class="existingWikiWord" href="/nlab/show/relative+entropy">relative entropy</a></p>
</li>
</ul>

<h2 id="information_geometry">Information geometry</h2>

<ul>
<li>
<p><a class="existingWikiWord" href="/nlab/show/information+geometry">information geometry</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/information+metric">information metric</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/Wasserstein+metric">Wasserstein metric</a></p>
</li>
</ul>

<h2 id="thermodynamics">Thermodynamics</h2>

<ul>
<li>
<p><a class="existingWikiWord" href="/nlab/show/thermodynamics">thermodynamics</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/second+law+of+thermodynamics">second law of thermodynamics</a>, <a class="existingWikiWord" href="/nlab/show/generalized+second+law+of+theormodynamics">generalized second law</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/ergodic+theory">ergodic theory</a></p>
</li>
</ul>

<h2 id="theorems">Theorems</h2>

<ul>
<li><a class="existingWikiWord" href="/nlab/show/Riesz+representation+theorem">Riesz representation theorem</a></li>
</ul>

<h2 id="applications">Applications</h2>

<ul>
<li><a class="existingWikiWord" href="/nlab/show/machine+learning">machine learning</a>, <a class="existingWikiWord" href="/nlab/show/neural+networks">neural networks</a></li>
</ul>
</div></div>
</div>
</div>

<h1 id="contents">Contents</h1>
<div class='maruku_toc'>
<ul>
<li><a href='#idea'>Idea</a></li>
<li><a href='#Definition'>Definition</a></li>
<li><a href='#relation_to_differential_equations_and_dynamical_systems'>Relation to differential equations and dynamical systems</a></li>
<li><a href='#RelationToRenormalizationGroupFlow'>Relation to renormalization group flow</a></li>
<li><a href='#related_concepts'>Related concepts</a></li>
<li><a href='#references'>References</a></li>
<ul>
<li><a href='#general'>General</a></li>
<li><a href='#relation_to_tensor_networks'>Relation to tensor networks</a></li>
<li><a href='#ReferencesRelationToRenormalizationGroupFlow'>Relation to renormalization group flow</a></li>
</ul>
</ul>
</div>

<h2 id="idea">Idea</h2>

<p>A <strong>neural network</strong> is a class of <a class="existingWikiWord" href="/nlab/show/functions">functions</a> used in both <a class="existingWikiWord" href="/nlab/show/supervised+learning"> supervised</a> and <span class="newWikiWord"> unsupervised<a href="/nlab/new/unsupervised+learning">?</a></span> <a class="existingWikiWord" href="/nlab/show/machine+learning">machine learning</a> to approximate a correspondence between samples in a dataset and their associated labels.</p>

<h2 id="Definition">Definition</h2>

<div class="num_defn">
<h6 id="definition_2">Definition</h6>

<p>Where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><mi>K</mi><mo>⊂</mo><msup><mi>ℝ</mi> <mi>d</mi></msup></mrow><annotation encoding="application/x-tex">K\subset \mathbb{R}^d</annotation></semantics></math> is compact, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>T</mi> <mi>L</mi></msub><msub><mo stretchy="false">}</mo> <mrow><mi>L</mi><mo>≤</mo><mi>N</mi><mo>∈</mo><mi>ℕ</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\{T_L\}_{L\leq N \in \mathbb{N}}</annotation></semantics></math> a finite set of affine maps such that <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><msub><mi>T</mi> <mi>L</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>W</mi> <mi>L</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">⟩</mo><mo>+</mo><msub><mi>b</mi> <mi>L</mi></msub></mrow><annotation encoding="application/x-tex">T_L(x) = \langle W_L,x\rangle + b_L</annotation></semantics></math> where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><msub><mi>W</mi> <mi>L</mi></msub></mrow><annotation encoding="application/x-tex">W_L</annotation></semantics></math> is the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><msup><mi>L</mi> <mi>th</mi></msup></mrow><annotation encoding="application/x-tex">L^{th}</annotation></semantics></math> layer weight matrix and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><msub><mi>b</mi> <mi>L</mi></msub></mrow><annotation encoding="application/x-tex">b_L</annotation></semantics></math> the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><msup><mi>L</mi> <mi>th</mi></msup></mrow><annotation encoding="application/x-tex">L^{th}</annotation></semantics></math> layer bias, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><mi>g</mi><mo>:</mo><mi>ℝ</mi><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">g:\mathbb{R}\to\mathbb{R}</annotation></semantics></math> a non-linear activation function, a neural network is a function <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><mi>f</mi><mo>:</mo><mi>K</mi><mo>⊂</mo><msup><mi>ℝ</mi> <mi>d</mi></msup><mo>→</mo><msup><mi>ℝ</mi> <mi>m</mi></msup></mrow><annotation encoding="application/x-tex">f:K\subset \mathbb{R}^d \to \mathbb{R}^m</annotation></semantics></math>, such that on input <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math>, computes the composition:</p>
<div class="maruku-equation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block" class="maruku-mathml"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><msub><mi>T</mi> <mi>L</mi></msub><mo>∘</mo><mi>g</mi><mo>∘</mo><msub><mi>T</mi> <mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>∘</mo><mi>g</mi><mo>∘</mo><mi>…</mi><mo>∘</mo><msub><mi>T</mi> <mn>1</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x) = (T_L\circ g \circ T_{L-1}\circ g \circ \dots \circ T_1)(x)</annotation></semantics></math></div>
<p>where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math> is applied component-wise.</p>
</div>

<p>Typically, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><msub><mi>T</mi> <mn>1</mn></msub></mrow><annotation encoding="application/x-tex">T_1</annotation></semantics></math> is called the input layer, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><msub><mi>T</mi> <mi>L</mi></msub></mrow><annotation encoding="application/x-tex">T_L</annotation></semantics></math> the output layer, and layers <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><msub><mi>T</mi> <mn>2</mn></msub></mrow><annotation encoding="application/x-tex">T_2</annotation></semantics></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><msub><mi>T</mi> <mrow><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">T_{L-1}</annotation></semantics></math> are hidden layers. In particular, a real-valued 1-hidden layer neural network with computes:</p>
<div class="maruku-equation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block" class="maruku-mathml"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>b</mi><mo>′</mo><mo lspace="verythinmathspace" rspace="0em">+</mo><munderover><mo lspace="thinmathspace" rspace="thinmathspace">∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover><msub><mi>a</mi> <mi>i</mi></msub><mi>g</mi><mo stretchy="false">(</mo><mo stretchy="false">⟨</mo><msub><mi>W</mi> <mi>i</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">⟩</mo><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x) = b' + \sum_{i=1}^n a_i g(\langle W_i, x\rangle + b)</annotation></semantics></math></div>
<p>where <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><mi>a</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>a</mi> <mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>a</mi> <mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a = (a_1, \dots, a_n)</annotation></semantics></math> is the output weight, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><mi>b</mi><mo>′</mo></mrow><annotation encoding="application/x-tex">b'</annotation></semantics></math> the output bias, <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><msub><mi>W</mi> <mi>i</mi></msub></mrow><annotation encoding="application/x-tex">W_i</annotation></semantics></math> the <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><msup><mi>i</mi> <mi>th</mi></msup></mrow><annotation encoding="application/x-tex">i^{th}</annotation></semantics></math> row of the hidden weight matrix, and <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math> the hidden bias. Here, the hidden layer is <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline" class="maruku-mathml"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math>-dimensional.</p>

<h2 id="relation_to_differential_equations_and_dynamical_systems">Relation to differential equations and dynamical systems</h2>

<p>A relation between deep neural networks, <a class="existingWikiWord" href="/nlab/show/differential+equations">differential equations</a>, and <a class="existingWikiWord" href="/nlab/show/dynamical+systems">dynamical systems</a> was proposed in (<a href="#CMHRBH17">CMHRBH17</a>, <a href="#LZLD17">LZLD17</a>, <a href="#Weinan17">Weinan17</a>)</p>

<p>Victor Lopez-Pastor and Florian Marquardt proposed that certain <span class="newWikiWord">time-reversible<a href="/nlab/new/time-reversible">?</a></span> <span class="newWikiWord">Hamiltonian systems<a href="/nlab/new/Hamiltonian+systems">?</a></span> exhibit self-learning behaviour and a physical version of the <a class="existingWikiWord" href="/nlab/show/backpropagation">backpropagation</a> <a class="existingWikiWord" href="/nlab/show/algorithm">algorithm</a>.</p>

<h2 id="RelationToRenormalizationGroupFlow">Relation to renormalization group flow</h2>

<p>A relation between deep neural networks (DNNs) based on Restricted Boltzmann Machines (RBMs) and <a class="existingWikiWord" href="/nlab/show/renormalization+group+flow">renormalization group flow</a> in <a class="existingWikiWord" href="/nlab/show/physics">physics</a> was proposed in (<a href="#MS14">MS14</a>).</p>

<h2 id="related_concepts">Related concepts</h2>

<ul>
<li>
<p><a class="existingWikiWord" href="/nlab/show/function+machine">function machine</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/tensor+network">tensor network</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/string+diagram">string diagram</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/physical+neural+network">physical neural network</a></p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/spiking+neural+network">spiking neural network</a></p>
</li>
</ul>

<h2 id="references">References</h2>

<h3 id="general">General</h3>

<p>Textbook account:</p>

<ul>
<li>Daniel A. Roberts, Sho Yaida, Boris Hanin, <em>The Principles of Deep Learning Theory</em>, Cambridge University Press 2022 (<a href="https://arxiv.org/abs/2106.10165">arXiv:2106.10165</a>)</li>
</ul>

<p>On the learning algorithm as analogous to differential equations and dynamical systems:</p>

<ul>
<li id="CMHRBH17">
<p>Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, Elliot Holtham, <em>Reversible Architectures for Arbitrarily Deep Residual Neural Networks</em>, (<a href="https://arxiv.org/abs/1709.03698">arXiv:1709.03698</a>)</p>
</li>

<li id="LZLD17">
<p>Yiping Lu, Aoxiao Zhong, Quanzheng Li, Bin Dong, <em>Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations</em>, (<a href="https://arxiv.org/abs/1710.10121">arXiv:1710.10121</a>).</p>
</li>

<li id="Weinan17">
<p>Weinan E, <em>A Proposal on Machine Learning via Dynamical Systems</em>, Communications in Mathematics and Statistics, 5, 1–11 (2017). (<a href="https://doi.org/10.1007/s40304-017-0103-z">doi:10.1007/s40304-017-0103-z</a>)</p>
</li>

<li>
<p>Victor Lopez-Pastor, Florian Marquardt, <em>Self-learning Machines based on Hamiltonian Echo Backpropagation</em>, (<a href="https://arxiv.org/abs/2103.04992">arXiv:2103.04992</a>)</p>
</li>
</ul>

<p>On the learning algorithm as <a class="existingWikiWord" href="/nlab/show/gradient+descent">gradient descent</a> of the loss functional:</p>

<ul>
<li id="ThierryMieg18">Jean Thierry-Mieg, <em>Connections between physics, mathematics and deep learning</em>, Letters in High Energy Physics, vol 2 no 3 (2019) (<a href="https://arxiv.org/abs/1811.00576">arXiv:1811.00576</a>, <a href="https://doi.org/10.31526/lhep.3.2019.110">doi:10.31526/lhep.3.2019.110</a>)</li>
</ul>

<p>On the learning algorithm as analogous to the <a class="existingWikiWord" href="/nlab/show/AdS%2FCFT+correspondence">AdS/CFT correspondence</a>:</p>

<ul>
<li>
<p>Yi-Zhuang You, Zhao Yang, Xiao-Liang Qi, <em>Machine Learning Spatial Geometry from Entanglement Features</em>, Phys. Rev. B 97, 045153 (2018) (<a href="https://arxiv.org/abs/1709.01223">arxiv:1709.01223</a>)</p>
</li>

<li>
<p>W. C. Gan and F. W. Shu, <em>Holography as deep learning</em>, Int. J. Mod. Phys. D 26, no. 12, 1743020 (2017) (<a href="https://arxiv.org/abs/1705.05750">arXiv:1705.05750</a>)</p>
</li>

<li>
<p>J. W. Lee, <em>Quantum fields as deep learning</em> (<a href="https://arxiv.org/abs/1708.07408">arXiv:1708.07408</a>)</p>
</li>

<li id="HSTT18">
<p><a class="existingWikiWord" href="/nlab/show/Koji+Hashimoto">Koji Hashimoto</a>, Sotaro Sugishita, Akinori Tanaka, Akio Tomiya, <em>Deep Learning and AdS/CFT</em>, Phys. Rev. D 98, 046019 (2018) (<a href="https://arxiv.org/abs/1802.08313">arxiv:1802.08313</a>)</p>
</li>
</ul>

<p><a class="existingWikiWord" href="/nlab/show/category+theory">Category theoretic</a> treatments of deep learning in neural networks:</p>

<ul>
<li>
<p><a class="existingWikiWord" href="/nlab/show/Brendan+Fong">Brendan Fong</a>, <a class="existingWikiWord" href="/nlab/show/David+Spivak">David Spivak</a>, Rémy Tuyéras, <em>Backprop as Functor: A compositional perspective on supervised learning</em>, (<a href="https://arxiv.org/abs/1711.10455">arXiv:1711.10455</a>)</p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/David+Spivak">David Spivak</a>, <em>Learners’ languages</em>, (<a href="https://arxiv.org/abs/2103.01189">arXiv:2103.01189</a>)</p>
</li>

<li>
<p>G.S.H. Cruttwell, <a class="existingWikiWord" href="/nlab/show/Bruno+Gavranovi%C4%87">Bruno Gavranović</a>, <a class="existingWikiWord" href="/nlab/show/Neil+Ghani">Neil Ghani</a>, Paul Wilson, Fabio Zanasi, <em>Categorical Foundations of Gradient-Based Learning</em>, (<a href="https://arxiv.org/abs/2103.01931">arXiv:2103.01931</a>)</p>
</li>
</ul>

<p>Quantum neural networks (in <a class="existingWikiWord" href="/nlab/show/quantum+computation">quantum computation</a> for <a class="existingWikiWord" href="/nlab/show/quantum+machine+learning">quantum machine learning</a>):</p>

<ul>
<li>
<p>Iris Cong, Soonwon Choi &amp; Mikhail D. Lukin, <em>Quantum convolutional neural networks</em>, Nature Physics volume 15, pages 1273–1278 (2019) (<a href="https://doi.org/10.1038/s41567-019-0648-8">doi:10.1038/s41567-019-0648-8</a>)</p>
</li>

<li id="MariBromleyIzaacSchuldKilloran20">
<p>Andrea Mari, Thomas R. Bromley, Josh Izaac, Maria Schuld, Nathan Killoran, <em>Transfer learning in hybrid classical-quantum neural networks</em>, Quantum 4, 340 (2020) (<a href="https://arxiv.org/abs/1912.08278">arXiv:1912.08278</a>)</p>
</li>

<li>
<p>Stefano Mangini, Francesco Tacchino, Dario Gerace, Daniele Bajoni, Chiara Macchiavello, <em>Quantum computing models for artificial neural networks</em>, EPL (Europhysics Letters) 134(1), 10002 (2021) (<a href="https://arxiv.org/abs/2102.03879">arXiv:2102.03879</a>)</p>
</li>
</ul>

<p>Topological deep learning for data supported on topological domains such as <a class="existingWikiWord" href="/nlab/show/simplicial+complexes">simplicial complexes</a>, <a class="existingWikiWord" href="/nlab/show/cell+complexes">cell complexes</a>, and <a class="existingWikiWord" href="/nlab/show/hypergraphs">hypergraphs</a>:</p>

<ul>
<li>
<p>Ephy R. Love, Benjamin Filippenko, Vasileios Maroulas, Gunnar Carlsson, <em>Topological Deep Learning</em> (<a href="https://arxiv.org/abs/2101.05778">arXiv:2101.05778</a>)</p>
</li>

<li>
<p>Mathilde Papillon, Sophia Sanborn, Mustafa Hajij, Nina Miolane, <em>Architectures of Topological Deep Learning: A Survey on Topological Neural Networks</em> (<a href="https://arxiv.org/abs/2304.10031">arXiv:2304.10031</a>)</p>
</li>

<li>
<p>Mustafa Hajij et al., <em>Topological Deep Learning: Going Beyond Graph Data</em> (<a href="https://www.researchgate.net/publication/370134352_Topological_Deep_Learning_Going_Beyond_Graph_Data">pdf</a>)</p>
</li>
</ul>

<h3 id="relation_to_tensor_networks">Relation to tensor networks</h3>

<p>Application of <a class="existingWikiWord" href="/nlab/show/tensor+networks">tensor networks</a> and specifically <a class="existingWikiWord" href="/nlab/show/tree+tensor+networks">tree tensor networks</a>:</p>

<ul>
<li>
<p>Ding Liu, Shi-Ju Ran, Peter Wittek, Cheng Peng, Raul Blázquez García, Gang Su, Maciej Lewenstein, <em>Machine Learning by Unitary Tensor Network of Hierarchical Tree Structure</em>, New Journal of Physics, 21, 073059 (2019) (<a href="https://arxiv.org/abs/1710.04833">arXiv:1710.04833</a>)</p>
</li>

<li>
<p>Song Cheng, Lei Wang, Tao Xiang, Pan Zhang, <em>Tree Tensor Networks for Generative Modeling</em>, Phys. Rev. B 99, 155131 (2019) (<a href="https://arxiv.org/abs/1901.02217">arXiv:1901.02217</a>)</p>
</li>
</ul>

<h3 id="ReferencesRelationToRenormalizationGroupFlow">Relation to renormalization group flow</h3>

<p>Relation to deep learning to <a class="existingWikiWord" href="/nlab/show/renormalization+group+flow">renormalization group flow</a>:</p>

<ul>
<li id="MS14">Pankaj Mehta, David J. Schwab - <em>An exact mapping between the Variational Renormalization Group and Deep Learning</em>, 2014 (<a href="https://arxiv.org/abs/1410.3831">arXiv:1410.3831</a>)</li>
</ul>

<p>Further discussion under the relation of <a class="existingWikiWord" href="/nlab/show/renormalization+group+flow">renormalization group flow</a> to <a class="existingWikiWord" href="/nlab/show/bulk">bulk</a>-flow in the context of the <a class="existingWikiWord" href="/nlab/show/AdS%2FCFT+correspondence">AdS/CFT correspondence</a>:</p>

<ul>
<li>
<p>Yi-Zhuang You, Zhao Yang, Xiao-Liang Qi, <em>Machine Learning Spatial Geometry from Entanglement Features</em>, Phys. Rev. B 97, 045153 (2018) (<a href="https://arxiv.org/abs/1709.01223">arxiv:1709.01223</a>)</p>
</li>

<li>
<p>W. C. Gan and F. W. Shu, <em>Holography as deep learning</em>, Int. J. Mod. Phys. D 26, no. 12, 1743020 (2017) (<a href="https://arxiv.org/abs/1705.05750">arXiv:1705.05750</a>)</p>
</li>

<li>
<p>J. W. Lee, <em>Quantum fields as deep learning</em> (<a href="https://arxiv.org/abs/1708.07408">arXiv:1708.07408</a>)</p>
</li>

<li>
<p><a class="existingWikiWord" href="/nlab/show/Koji+Hashimoto">Koji Hashimoto</a>, Sotaro Sugishita, Akinori Tanaka, Akio Tomiya, <em>Deep Learning and AdS/CFT</em>, Phys. Rev. D 98, 046019 (2018) (<a href="https://arxiv.org/abs/1802.08313">arxiv:1802.08313</a>)</p>
</li>
</ul>
</body></html>

</div>

<div class="revisedby">
    <p>
    Last revised on April 24, 2023 at 19:12:28.
    See the <a href="/nlab/history/neural+network" style="color: #005c19">history</a> of this page for a list of all contributions to it.
    </p>
</div>

<div class="navigation navfoot">

  <a href="/nlab/edit/neural+network" accesskey="E" class="navlink" id="edit" rel="nofollow">Edit</a><a href="https://nforum.ncatlab.org/discussion/10400/#Item_19">Discuss</a><span class="backintime"><a href="/nlab/revision/neural+network/21" accesskey="B" class="navlinkbackintime" id="to_previous_revision" rel="nofollow">Previous revision</a></span><a href="/nlab/show/diff/neural+network" accesskey="C" class="navlink" id="see_changes" rel="nofollow">Changes from previous revision</a><a href="/nlab/history/neural+network" accesskey="S" class="navlink" id="history" rel="nofollow">History (21 revisions)</a>
  <a href="/nlab/show/neural+network/cite" style="color: black">Cite</a>
  <a href="/nlab/print/neural+network" accesskey="p" id="view_print" rel="nofollow">Print</a>
    <a href="/nlab/source/neural+network" id="view_source" rel="nofollow">Source</a>

  


</div>


</div> <!-- Content -->

</div> <!-- Container -->

</body>
</html>
